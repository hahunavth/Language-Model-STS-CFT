{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9794355,"sourceType":"datasetVersion","datasetId":5997538},{"sourceId":9807760,"sourceType":"datasetVersion","datasetId":5999802}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Set env","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb_api\")\nwandb.login(key=wandb_api)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# SET ENV VARIABLE\nNote: This script only works when run on Kaggle notebook directly (not run with !python secret.py)\n\"\"\"\nexec(\"\"\"\\nimport os\\nfrom kaggle_secrets import UserSecretsClient\\n\\nSECRET_REPO_DIR='/secret'\\n\\nuser_secrets = UserSecretsClient()\\nprint(\"Get kaggle secret ...\")\\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\\n\\nos.system(f'rm -rf {SECRET_REPO_DIR};')\\nos.system(f'git clone https://hahunavth:{GITHUB_TOKEN}@github.com/hahunavth/kaggle-secret.git {SECRET_REPO_DIR}')\\n\\nprint(\"Load .env ...\")\\nwith open(f\"{SECRET_REPO_DIR}/kaggle.env\", \"r\") as f:\\n    for line in f.readlines():\\n        line = line.rstrip()\\n        if len(line) == 0:\\n            continue\\n        line = line.split(\"=\")\\n        if len(line) == 2:\\n            name, value = line\\n            print(f\"Set: {name}\")\\n            os.environ[name.rstrip()] = value.rstrip()\\n        elif len(line) > 0:\\n            print(f\"Ignore: {line[0]}, invalid syntax\")\\n\\nprint(\"Remove repo ...\")\\nos.system(f'rm -rf {SECRET_REPO_DIR};')\\nprint(\"Done\")\\n\\nassert os.environ[name]\\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clone repo","metadata":{}},{"cell_type":"code","source":"PROJ_DIR = \"/kaggle/Language-Model-STS-CFT\"\n\n%cd /kaggle\n!git clone https://github.com/trapoom555/Language-Model-STS-CFT\n\n!mkdir $PROJ_DIR/pretrained\n!mkdir /kaggle/working/out -p\n!ln -s /kaggle/working/out $PROJ_DIR/train/output\n# !mkdir $PROJ_DIR/train/output","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download pretrained and change a tokenizer setting","metadata":{}},{"cell_type":"code","source":"# openbmb/MiniCPM-2B-dpo-bf16\n\n# %cd $PROJ_DIR/pretrained\n# !git clone https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16\n\n# pt_tokenizer_config_file = f\"{PROJ_DIR}/pretrained/MiniCPM-2B-dpo-bf16/tokenizer_config.json\"\n\n# import json\n\n# with open(pt_tokenizer_config_file, \"r\") as f:\n#     config = json.loads(f.read())\n#     config['add_eos_token'] = True\n\n# with open(pt_tokenizer_config_file, \"w\") as f:\n#     json.dump(config, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # google/gemma-2b-it\n\n# %cd $PROJ_DIR/pretrained\n# HF_TOKEN = os.environ['HF_TOKEN']\n# !git clone https://hahaunavth:$HF_TOKEN@huggingface.co/google/gemma-2b-it\n\n# pt_tokenizer_config_file = f\"{PROJ_DIR}/pretrained/gemma-2b-it/tokenizer_config.json\"\n\n# import json\n\n# with open(pt_tokenizer_config_file, \"r\") as f:\n#     config = json.loads(f.read())\n#     config['add_eos_token'] = True\n\n# with open(pt_tokenizer_config_file, \"w\") as f:\n#     json.dump(config, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # google/gemma-2-2b-it\n\n# %cd $PROJ_DIR/pretrained\n# HF_TOKEN = os.environ['HF_TOKEN']\n# !git clone https://hahaunavth:$HF_TOKEN@huggingface.co/google/gemma-2-2b-it\n\n# pt_tokenizer_config_file = f\"{PROJ_DIR}/pretrained/gemma-2-2b-it/tokenizer_config.json\"\n\n# import json\n\n# with open(pt_tokenizer_config_file, \"r\") as f:\n#     config = json.loads(f.read())\n#     config['add_eos_token'] = True\n\n# with open(pt_tokenizer_config_file, \"w\") as f:\n#     json.dump(config, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# google/gemma-2-2b-jpn-it\n\n%cd $PROJ_DIR/pretrained\nHF_TOKEN = os.environ['HF_TOKEN']\n!git clone https://hahaunavth:$HF_TOKEN@huggingface.co/google/gemma-2-2b-jpn-it\n\npt_tokenizer_config_file = f\"{PROJ_DIR}/pretrained/gemma-2-2b-jpn-it/tokenizer_config.json\"\n\nimport json\n\nwith open(pt_tokenizer_config_file, \"r\") as f:\n    config = json.loads(f.read())\n    config['add_eos_token'] = True\n\nwith open(pt_tokenizer_config_file, \"w\") as f:\n    json.dump(config, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download and preprocess data","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/Language-Model-STS-CFT/data\n\n# ENGLISH DATASET\n# !./download_nli.sh\n# !python nli_preprocess.py\n\n# # MU-Kindai/datasets-for-JCSE\n# !git clone https://huggingface.co/datasets/MU-Kindai/datasets-for-JCSE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls datasets-for-JCSE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n\n# df = pd.read_csv('./datasets-for-JCSE/clinic_shuffle_for_simcse_top5.csv')\n# df[~df['hard_neg'].isnull()].to_csv('./datasets-for-JCSE/clinic_shuffle_for_simcse_top5_filtered.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/jsnli-for-simcse/jsnli_for_simcse.csv\")\ndf['sent0'] = df['sent0'].apply(lambda x: x.replace(' ', \"\"))\ndf['sent1'] = df['sent1'].apply(lambda x: x.replace(' ', \"\"))\ndf['hard_neg'] = df['hard_neg'].apply(lambda x: x.replace(' ', \"\"))\n\ndf = df[:100000] # first 100000 row -> train set\n\ndf.to_csv(\"jsnli_for_simcse_cleaned.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile nli_preprocess_jcse.py\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\nclass NLIPreprocess:\n    def __init__(self, path, tokenizer_path):\n        self.ds = load_dataset(\"csv\", data_files=path)['train']\n\n#         tokenizer_path = '../pretrained/MiniCPM-2B-dpo-bf16/'\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self._preprocess()\n    \n    def _tokenize(self, text, id):\n\n        out = self.tokenizer(text, \n                    padding='max_length', \n                    truncation=True, \n                    return_tensors=\"pt\", \n                    max_length=150)\n        \n        out[id + '_input_ids'] = out.pop('input_ids')\n        out[id + '_attention_mask'] = out.pop('attention_mask')\n\n        return out\n    \n    def _preprocess(self):\n        self.ds = self.ds.map(\n            lambda x: self._tokenize(x['sent0'], 'sent0'), batched=True)\n        self.ds = self.ds.map(\n            lambda x: self._tokenize(x['sent1'], 'sent1'), batched=True)\n        self.ds = self.ds.map(\n            lambda x: self._tokenize(x['hard_neg'], 'hard_neg'), batched=True)\n        self.ds.set_format(\n            type=\"torch\", \n            columns=[\"sent0_input_ids\", \"sent0_attention_mask\",\n                     \"sent1_input_ids\", \"sent1_attention_mask\",\n                     \"hard_neg_input_ids\", \"hard_neg_attention_mask\",]\n        )\n\n# nlip = NLIPreprocess('./datasets-for-JCSE/clinic_shuffle_for_simcse_top5_filtered.csv', tokenizer_path='../pretrained/gemma-2-2b-jpn-it/')\n# nlip = NLIPreprocess('./datasets-for-JCSE/nli_for_simcse.csv', tokenizer_path='../pretrained/gemma-2-2b-jpn-it/')\n# nlip = NLIPreprocess('./nli_for_simcse.csv', tokenizer_path='../pretrained/gemma-2b-it/')\n# nlip = NLIPreprocess('./nli_for_simcse.csv', tokenizer_path='../pretrained/gemma-2-2b-it/')\n\n# nlip = NLIPreprocess('./nli_for_simcse.csv', tokenizer_path='../pretrained/gemma-2-2b-jpn-it/')\n\n# nlip = NLIPreprocess('/kaggle/input/jsnli-for-simcse/jsnli_for_simcse.csv', tokenizer_path='../pretrained/gemma-2-2b-jpn-it/')\nnlip = NLIPreprocess('./jsnli_for_simcse_cleaned.csv', tokenizer_path='../pretrained/gemma-2-2b-jpn-it/')\n\n\nnlip.ds.save_to_disk(\"./processed/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python nli_preprocess_jcse.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_from_disk\n\nds = load_from_disk(f\"{PROJ_DIR}/data/processed\")\nds[500]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('../pretrained/gemma-2-2b-jpn-it/', local_files_only=True)\ntokenizer.decode(ds[245]['hard_neg_input_ids'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finetuning","metadata":{}},{"cell_type":"code","source":"# !cat $PROJ_DIR/train/configs/ddp_config.yaml","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/Language-Model-STS-CFT/train/configs/ddp_config.yaml\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndistributed_type: MULTI_GPU\ndowncast_bf16: 'no'\nenable_cpu_affinity: false\ngpu_ids: all\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2 # 4\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install peft -q # ==0.10.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd $PROJ_DIR/train\n# !chmod +x train.sh\n# !./train.sh\n\n# %%bash\n# formatted_time=$(date +\"%Y%m%d%H%M%S\")\n# echo $formatted_time\n\n# accelerate launch --config_file ./configs/ddp_config.yaml train.py \\\n# --output_dir output/$formatted_time/ \\\n# --model_name_or_path ../pretrained/MiniCPM-2B-dpo-bf16/ \\\n# --temperature 0.05 \\\n# --train_data_path ../data/processed \\\n# --learning_rate 5e-5 \\\n# --per_device_train_batch_size 7 \\\n# --bf16 \\\n# --gradient_accumulation_steps 1 \\\n# --warmup_steps 100 \\\n# --max_steps 1000 \\\n# --weight_decay 1e-4 \\\n# --lr_scheduler_type \"cosine\" \\\n# --lora_r 8 --lora_alpha 32 --lora_dropout 0.1 \\\n# --save_strategy steps --save_steps 500 --seed 7 \\\n# --remove_unused_columns False \\\n# --log_level info --logging_strategy steps --logging_steps 10 --report_to wandb \\","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile train.py\n\nimport os\nimport torch\nimport transformers\nfrom typing import Optional\nfrom datasets import load_from_disk\nfrom dataclasses import dataclass, field\nfrom contrastive_trainer import ContrastiveTrainer\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import AutoModelForCausalLM, HfArgumentParser, set_seed\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    lora_alpha: Optional[int] = field(default=32)\n    lora_dropout: Optional[float] = field(default=0.1)\n    lora_r: Optional[int] = field(default=8)\n    lora_target_modules: Optional[str] = field(\n        default=\"q_proj,v_proj\",\n        metadata={\"help\": \"comma separated list of target modules to apply LoRA layers to\"},\n    )\n\n@dataclass\nclass DataArguments:\n    train_data_path: str = field(\n        metadata={\"help\": \"Path to training data\"}\n    )\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    temperature: Optional[float] = field(default=0.05)\n\ndef main(model_args, data_args, training_args):\n    set_seed(training_args.seed)\n\n    # Model\n    model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, \n                                                torch_dtype=torch.bfloat16,\n                                                trust_remote_code=True,\n                                                local_files_only=True)\n\n    # PEFT\n    lora_config = LoraConfig(init_lora_weights=\"gaussian\",\n                            task_type=TaskType.CAUSAL_LM,\n                            target_modules=[\"q_proj\", \"v_proj\"],\n                            r=model_args.lora_r,\n                            lora_alpha=model_args.lora_alpha,\n                            lora_dropout=model_args.lora_dropout,\n                            inference_mode=False)\n\n    model = get_peft_model(model, lora_config)\n\n    # Data\n    train_dataset = load_from_disk(data_args.train_data_path)\n\n    trainer = ContrastiveTrainer(model=model,\n                                args=training_args,\n                                train_dataset=train_dataset)\n\n    trainer.accelerator.print(f\"{trainer.model}\")\n    trainer.model.print_trainable_parameters()\n\n    # Train\n    checkpoint = \"/kaggle/input/llm-gemma-2-2b-jpn-it-finetune/out/20241104132512/checkpoint-500\" # None\n    if training_args.resume_from_checkpoint is not None:\n        checkpoint = training_args.resume_from_checkpoint\n    trainer.train(resume_from_checkpoint=checkpoint)\n\n    # Saving final model\n    trainer.save_model(training_args.output_dir)\n\nif __name__ == \"__main__\":\n    os.environ[\"WANDB_PROJECT\"] = \"minicpm-dense-retrieval\"\n    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n    main(model_args, data_args, training_args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%bash\nformatted_time=$(date +\"%Y%m%d%H%M%S\")\necho $formatted_time\n\n# --model_name_or_path ../pretrained/MiniCPM-2B-dpo-bf16/ \\\n# --model_name_or_path ../pretrained/gemma-2-2b-jpn-it/ \\\n# --model_name_or_path ../pretrained/gemma-2b-it/ \\\n# --model_name_or_path ../pretrained/gemma-2-2b-it/ \\\n\n# --per_device_train_batch_size 4 \\\n# --gradient_accumulation_steps 1 \\\n# --max_steps 1000\n# --learning_rate 5e-5 \\\n# --weight_decay 1e-4 \\\n\n\n# export CUDA_LAUNCH_BLOCKING=1;\n# export TORCH_USE_CUDA_DSA=1;\naccelerate launch --config_file ./configs/ddp_config.yaml train.py \\\n--output_dir output/$formatted_time/ \\\n--model_name_or_path ../pretrained/gemma-2-2b-jpn-it/ \\\n--temperature 0.05 \\\n--train_data_path ../data/processed \\\n--learning_rate 5e-5 \\\n--per_device_train_batch_size 4 \\\n--bf16 \\\n--gradient_accumulation_steps 8 \\\n--warmup_steps 100 \\\n--max_steps 1000 \\\n--weight_decay 8e-4 \\\n--lr_scheduler_type \"cosine\" \\\n--lora_r 8 --lora_alpha 32 --lora_dropout 0.1 \\\n--save_strategy steps --save_steps 500 --seed 7 \\\n--remove_unused_columns False \\\n--log_level info --logging_strategy steps --logging_steps 10 --report_to wandb \\\n--max_grad_norm 1.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}